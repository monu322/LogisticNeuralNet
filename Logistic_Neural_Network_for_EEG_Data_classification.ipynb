{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic Neural Network for EEG Data classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmqAymwC2zIA"
      },
      "source": [
        "from scipy.io import loadmat\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nikqippJ3SUk",
        "outputId": "794abdce-f400-4691-ea99-21980cdb8561"
      },
      "source": [
        "!wget https://monujohn.com/EEGdata/WLDataAll.mat"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-04 07:49:42--  https://monujohn.com/EEGdata/WLDataAll.mat\n",
            "Resolving monujohn.com (monujohn.com)... 198.136.51.114\n",
            "Connecting to monujohn.com (monujohn.com)|198.136.51.114|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42375907 (40M)\n",
            "Saving to: ‘WLDataAll.mat’\n",
            "\n",
            "WLDataAll.mat       100%[===================>]  40.41M  13.6MB/s    in 3.0s    \n",
            "\n",
            "2021-08-04 07:49:45 (13.6 MB/s) - ‘WLDataAll.mat’ saved [42375907/42375907]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ede5j64_3Yv-"
      },
      "source": [
        "annots = loadmat('WLDataAll.mat')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkOnxfYn3mzq"
      },
      "source": [
        "data = annots['data']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAPdXkXO3ulT",
        "outputId": "02f0f914-b8a8-4105-d8d7-3d718653fc62"
      },
      "source": [
        "data_transpose = data[0].T\n",
        "print('Original shape:', data_transpose.shape)\n",
        "\n",
        "data_squared = np.square(data_transpose)\n",
        "data_sum = data_squared.sum(axis=1)\n",
        "\n",
        "\n",
        "print('New shape:', data_sum.shape)\n",
        "\n",
        "def NormalizeData(data):\n",
        "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
        "\n",
        "\n",
        "data_normalized = NormalizeData(data_sum)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original shape: (360, 512)\n",
            "New shape: (360,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szwzDBWd53Vp",
        "outputId": "008cb18c-0dc7-4c27-ca25-37423bc8c5ba"
      },
      "source": [
        "datapoint_array = []\n",
        "\n",
        "for datapoint in data:\n",
        "  datapoint = datapoint.T\n",
        "  \n",
        "  data_sum_squared = np.square(datapoint)\n",
        "  \n",
        "  data_sum = data_sum_squared.sum(axis=1)\n",
        "  \n",
        "  data_normalized = NormalizeData(data_sum)\n",
        "  datapoint_array.append(data_normalized)\n",
        "\n",
        "\n",
        "datapoint_array = np.array(datapoint_array)\n",
        "\n",
        "print('Latest array shape:', datapoint_array.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Latest array shape: (62, 360)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68bNUVBI-0M2",
        "outputId": "a7b8c0ff-83cb-4eb7-da7a-4c4c9cf59a14"
      },
      "source": [
        "X = datapoint_array\n",
        "Y = annots['label']\n",
        "\n",
        "Y = np.where(Y == 1, 0, Y)\n",
        "Y = np.where(Y == 2, 1, Y)\n",
        "\n",
        "print('X shape:', X.shape)\n",
        "print('Y shape:', Y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X shape: (62, 360)\n",
            "Y shape: (1, 360)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmME7JDf_Oe4"
      },
      "source": [
        "class ActivationDoesNotExist(Exception):\n",
        "    \"\"\"Valid activations are sigmoid, tanh, and relu, provided as a string\"\"\"\n",
        "    pass\n",
        "\n",
        "class InputDimensionNotCorrect(Exception):\n",
        "    \"\"\"Need to specify input dimension, i.e. input shape into the first layer\"\"\"\n",
        "    pass\n",
        "\n",
        "class LossFunctionNotDefined(Exception):\n",
        "    \"\"\"Loss function in cost() method not defined\"\"\"\n",
        "    pass\n",
        "\n",
        "\n",
        "class DenseLayer:\n",
        "    def __init__(self, inputDimension, units, activation='', randomMultiplier=0.01):\n",
        "        self.weights, self.bias = self.initialize(inputDimension, units, randomMultiplier)\n",
        "        if activation == 'sigmoid':\n",
        "            self.activation = activation\n",
        "            self.activationForward = self.sigmoid\n",
        "            self.activationBackward = self.sigmoidGrad\n",
        "        elif activation == 'relu':\n",
        "            self.activation = activation\n",
        "            self.activationForward = self.relu\n",
        "            self.activationBackward = self.reluGrad\n",
        "        elif activation == 'tanh':\n",
        "            self.activation = activation\n",
        "            self.activationForward = self.tanh\n",
        "            self.activationBackward = self.tanhGrad\n",
        "        elif activation != '':\n",
        "            raise ActivationDoesNotExist\n",
        "        else:\n",
        "            self.activation = 'none'\n",
        "            self.activationForward = self.linear\n",
        "            self.activationBackward = self.linear\n",
        "\n",
        "    def initialize(self, nx, nh, randomMultiplier):\n",
        "        weights = randomMultiplier * np.random.randn(nh, nx)\n",
        "        bias = np.zeros([nh, 1])\n",
        "        return weights, bias\n",
        "\n",
        "    def sigmoid(self, Z):\n",
        "        \"\"\"\n",
        "        Sigmoid activation function\n",
        "        \"\"\"\n",
        "        A = 1 / (1 + np.exp(-Z))\n",
        "        return A\n",
        "        \n",
        "    def sigmoidGrad(self, dA):\n",
        "        \"\"\"\n",
        "        Differential of sigmoid function with chain rule applied\n",
        "        \"\"\"\n",
        "        s = 1 / (1 + np.exp(-self.prevZ))\n",
        "        dZ = dA * s * (1 - s)\n",
        "        return dZ\n",
        "    \n",
        "    \n",
        "    def relu(self, Z):\n",
        "        \"\"\"\n",
        "        Relu activation function\n",
        "        \"\"\"\n",
        "        A = np.maximum(0, Z)\n",
        "        return A\n",
        "  \n",
        "    def reluGrad(self, dA):\n",
        "        \"\"\"\n",
        "        Differential of relu function with chain rule applied\n",
        "        \"\"\"\n",
        "        s = np.maximum(0, self.prevZ)\n",
        "        dZ = (s>0) * 1 * dA\n",
        "        return dZ \n",
        "\n",
        "        \n",
        "    def tanh(self, Z):\n",
        "        \"\"\"\n",
        "        Tanh activation function\n",
        "        \"\"\"\n",
        "        A = np.tanh(Z)\n",
        "        return A\n",
        "\n",
        "    def tanhGrad(self, dA):\n",
        "        \"\"\"\n",
        "        Differential of tanh function with chain rule applied\n",
        "        \"\"\"\n",
        "        s = np.tanh(self.prevZ)\n",
        "        dZ = (1 - s**2) * dA\n",
        "        return dZ\n",
        "\n",
        "\n",
        "    def linear(self, Z):\n",
        "        \"\"\"\n",
        "        Placeholder when no activation function is used\n",
        "        \"\"\"\n",
        "        return Z\n",
        "        \n",
        "    \n",
        "    def forward(self, A):\n",
        "        \"\"\"\n",
        "        Forward pass through layer\n",
        "          A: input vector\n",
        "        \"\"\"\n",
        "        Z = np.dot(self.weights, A) + self.bias\n",
        "        self.prevZ = Z\n",
        "        self.prevA = A\n",
        "        A = self.activationForward(Z)\n",
        "        return A\n",
        "    \n",
        "    \n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        Backward pass through layer\n",
        "          dA: previous gradient\n",
        "        \"\"\"\n",
        "        dZ = self.activationBackward(dA)\n",
        "        m = self.prevA.shape[1]\n",
        "        self.dW = 1 / m * np.dot(dZ, self.prevA.T)\n",
        "        self.db = 1 / m * np.sum(dZ, axis=1, keepdims=True)\n",
        "        prevdA = np.dot(self.weights.T, dZ)\n",
        "        return prevdA\n",
        "    \n",
        "    \n",
        "    def update(self, learning_rate):\n",
        "        \"\"\"\n",
        "        Update weights using gradients from backward pass\n",
        "          learning_rate: the learning rate used in the gradient descent\n",
        "        \"\"\"\n",
        "        self.weights = self.weights - learning_rate * self.dW\n",
        "        self.bias = self.bias - learning_rate * self.db\n",
        "\n",
        "        \n",
        "    def outputDimension(self):\n",
        "        \"\"\"\n",
        "        Returns the output dimension for the next layer\n",
        "        \"\"\"\n",
        "        return len(self.bias)\n",
        "\n",
        "    \n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        Used to print a pretty summary of the layer\n",
        "        \"\"\"\n",
        "        act = 'none' if self.activation == '' else self.activation\n",
        "        return f'Dense layer (nx={self.weights.shape[1]}, nh={self.weights.shape[0]}, activation={act})'\n",
        "\n",
        "\n",
        "class NeuralNetwork:\n",
        "    \"\"\"\n",
        "    Neural Network structure that holds our layers\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, loss='cross-entropy', randomMultiplier = 0.01):\n",
        "        \"\"\"\n",
        "        Constructor:\n",
        "          loss: the loss function. Two are defined:\n",
        "             - 'cross-entropy' and 'mean-square-error'\n",
        "          randomMultiplier: multiplier applied to the random weights during initialization\n",
        "        \"\"\"\n",
        "        self.layers=[]\n",
        "        self.randomMultiplier = randomMultiplier\n",
        "        if loss=='cross-entropy':\n",
        "            self.lossFunction = self.crossEntropyLoss\n",
        "            self.lossBackward = self.crossEntropyLossGrad\n",
        "        elif loss=='mean-square-error':\n",
        "            self.lossFunction = self.meanSquareError\n",
        "            self.lossBackward = self.meanSquareErrorGrad\n",
        "        else:\n",
        "            raise LossFunctionNotDefined\n",
        "        self.loss=loss\n",
        "\n",
        "\n",
        "    def addLayer(self, inputDimension=None, units=1, activation=''):\n",
        "        \"\"\"\n",
        "        Adds a Dense layer to the network:\n",
        "          inputDimension: required when it is the first layer. otherwise takes dimensions of previous layer.\n",
        "          units: number of neurons in the layer\n",
        "          activation: activation function: valid choices are: 'sigmoid', 'tanh', 'relu', ''\n",
        "        \"\"\"\n",
        "        if (inputDimension is None):\n",
        "            if (len(self.layers)==0):\n",
        "                raise InputDimensionNotCorrect\n",
        "            inputDimension=self.layers[-1].outputDimension()\n",
        "        layer = DenseLayer(inputDimension, units, activation, randomMultiplier= self.randomMultiplier)\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def crossEntropyLoss(self, Y, A, epsilon=1e-15):\n",
        "        \"\"\"\n",
        "        Cross Entropy loss function\n",
        "          Y: true labels\n",
        "          A: final activation function (predicted labels)\n",
        "          epsilon: small value to make minimize chance for log(0) error\n",
        "        \"\"\"\n",
        "        m = Y.shape[1]\n",
        "        loss = -1 * (Y * np.log(A + epsilon) + (1 - Y) * np.log(1 - A + epsilon))\n",
        "        cost = 1 / m * np.sum(loss)\n",
        "        return np.squeeze(cost)\n",
        "            \n",
        "    def crossEntropyLossGrad(self, Y, A):\n",
        "        \"\"\"\n",
        "        Cross Entropy loss Gradient\n",
        "          Y: true labels\n",
        "          A: final activation function (predicted labels)\n",
        "        \"\"\"\n",
        "        dA = -(np.divide(Y, A) - np.divide(1 - Y, 1 - A))\n",
        "        return dA\n",
        "    \n",
        "    \n",
        "    def meanSquareError(self, Y, A):\n",
        "        \"\"\"\n",
        "        Mean square error loss function\n",
        "          Y: true labels\n",
        "          A: final activation function (predicted labels)\n",
        "        \"\"\"\n",
        "        loss = np.square(Y - A)\n",
        "        m = Y.shape[1]\n",
        "        cost = 1 / m * np.sum(loss)\n",
        "        return np.squeeze(cost)\n",
        "    \n",
        "    def meanSquareErrorGrad(self, Y, A):\n",
        "        \"\"\"\n",
        "        Mean square error loss gradient\n",
        "          Y: true labels\n",
        "          A: final activation function (predicted labels)\n",
        "        \"\"\"\n",
        "        dA = -2 * (Y - A)\n",
        "        return dA\n",
        "\n",
        "    \n",
        "    def cost(self, Y, A):\n",
        "        \"\"\"\n",
        "        Cost function wrapper\n",
        "          Y: true labels\n",
        "          A: final activation function (predicted labels)\n",
        "        \"\"\"\n",
        "        return self.lossFunction(Y, A)\n",
        "\n",
        "        \n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass through the whole model.\n",
        "          X: input vector\n",
        "        \"\"\"\n",
        "        x = np.copy(X)\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "            \n",
        "    \n",
        "    def backward(self, A, Y):\n",
        "        \"\"\"\n",
        "        backward pass through the whole model\n",
        "          Y: true labels\n",
        "          A: final activation function (predicted labels)\n",
        "        \"\"\"\n",
        "        dA = self.lossBackward(Y, A)\n",
        "        for layer in reversed(self.layers):\n",
        "            dA = layer.backward(dA)\n",
        "    \n",
        "    \n",
        "    def update(self, learning_rate=0.01):\n",
        "        \"\"\"\n",
        "        Update weights and do a step of gradient descent for the whole model.\n",
        "          learning_rate: learning_rate to use\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            layer.update(learning_rate)\n",
        "    \n",
        "    \n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        Pretty print the model\n",
        "        \"\"\"\n",
        "        layrepr = ['  ' + str(ix+1)+' -> ' + str(x) for ix, x in enumerate(self.layers)]\n",
        "        return '[\\n' + '\\n'.join(layrepr) + '\\n]'\n",
        "   \n",
        "    \n",
        "    def numberOfParameters(self):\n",
        "        \"\"\"\n",
        "        Print number of trainable parameters in the model\n",
        "        \"\"\"\n",
        "        n = 0\n",
        "        for layer in self.layers:\n",
        "            n += np.size(layer.weights) + len(layer.bias)\n",
        "        print(f'There are {n} trainable parameters in the model.')\n",
        "\n",
        "\n",
        "\n",
        "def roundValue(A):\n",
        "    return np.uint8( A > 0.5)\n",
        "\n",
        "def accuracy(yhat, Y):\n",
        "    return round(np.sum(yhat==Y) / len(yhat.flatten()) * 1000) / 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQGf6qC___Wg",
        "outputId": "72931f3c-7ab4-4327-81d8-182f500339e2"
      },
      "source": [
        "np.random.seed(1)\n",
        "\n",
        "model = NeuralNetwork()\n",
        "model.addLayer(inputDimension=62, units=1, activation='sigmoid')\n",
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\n",
              "  1 -> Dense layer (nx=62, nh=1, activation=sigmoid)\n",
              "]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VXV4-VRAFIn",
        "outputId": "7bd851fc-e2ed-4609-e075-2d3139581099"
      },
      "source": [
        "num_iterations = 1000000\n",
        "for ix in range(num_iterations):\n",
        "    A = model.forward(X)\n",
        "    model.backward(A, Y)\n",
        "    model.update()\n",
        "    if ix % 10000 == 0:\n",
        "        yhat = roundValue(A)\n",
        "        print('cost:', model.cost(Y, A), f'\\taccuracy: {accuracy(yhat, Y)}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cost: 0.6159966195103541 \taccuracy: 65.0%\n",
            "cost: 0.6157774137979796 \taccuracy: 65.0%\n",
            "cost: 0.6155603418650166 \taccuracy: 65.0%\n",
            "cost: 0.6153453636213386 \taccuracy: 65.0%\n",
            "cost: 0.6151324400390877 \taccuracy: 64.7%\n",
            "cost: 0.6149215331162987 \taccuracy: 64.7%\n",
            "cost: 0.614712605842079 \taccuracy: 64.7%\n",
            "cost: 0.614505622163264 \taccuracy: 64.7%\n",
            "cost: 0.6143005469524708 \taccuracy: 64.7%\n",
            "cost: 0.6140973459774802 \taccuracy: 64.7%\n",
            "cost: 0.613895985871878 \taccuracy: 64.4%\n",
            "cost: 0.6136964341068953 \taccuracy: 64.2%\n",
            "cost: 0.6134986589643832 \taccuracy: 64.2%\n",
            "cost: 0.613302629510873 \taccuracy: 64.2%\n",
            "cost: 0.6131083155726627 \taccuracy: 63.9%\n",
            "cost: 0.6129156877118852 \taccuracy: 63.9%\n",
            "cost: 0.6127247172035086 \taccuracy: 63.9%\n",
            "cost: 0.6125353760132252 \taccuracy: 63.9%\n",
            "cost: 0.6123476367761883 \taccuracy: 64.2%\n",
            "cost: 0.6121614727765571 \taccuracy: 64.2%\n",
            "cost: 0.6119768579278119 \taccuracy: 64.2%\n",
            "cost: 0.6117937667538061 \taccuracy: 64.2%\n",
            "cost: 0.6116121743705192 \taccuracy: 64.2%\n",
            "cost: 0.6114320564684832 \taccuracy: 64.4%\n",
            "cost: 0.6112533892958482 \taccuracy: 64.4%\n",
            "cost: 0.6110761496420635 \taccuracy: 64.4%\n",
            "cost: 0.6109003148221428 \taccuracy: 64.4%\n",
            "cost: 0.6107258626614923 \taccuracy: 64.7%\n",
            "cost: 0.6105527714812761 \taccuracy: 64.7%\n",
            "cost: 0.6103810200842938 \taccuracy: 64.7%\n",
            "cost: 0.6102105877413536 \taccuracy: 64.4%\n",
            "cost: 0.6100414541781148 \taccuracy: 63.9%\n",
            "cost: 0.6098735995623843 \taccuracy: 63.9%\n",
            "cost: 0.6097070044918458 \taccuracy: 63.9%\n",
            "cost: 0.6095416499822064 \taccuracy: 64.2%\n",
            "cost: 0.6093775174557414 \taccuracy: 64.2%\n",
            "cost: 0.6092145887302217 \taccuracy: 64.2%\n",
            "cost: 0.60905284600821 \taccuracy: 64.2%\n",
            "cost: 0.6088922718667102 \taccuracy: 64.2%\n",
            "cost: 0.6087328492471555 \taccuracy: 64.2%\n",
            "cost: 0.6085745614457223 \taccuracy: 64.2%\n",
            "cost: 0.6084173921039586 \taccuracy: 64.2%\n",
            "cost: 0.6082613251997128 \taccuracy: 64.2%\n",
            "cost: 0.6081063450383524 \taccuracy: 64.2%\n",
            "cost: 0.6079524362442617 \taccuracy: 64.2%\n",
            "cost: 0.6077995837526072 \taccuracy: 64.2%\n",
            "cost: 0.6076477728013621 \taccuracy: 64.2%\n",
            "cost: 0.6074969889235784 \taccuracy: 64.2%\n",
            "cost: 0.6073472179398983 \taccuracy: 64.2%\n",
            "cost: 0.6071984459512975 \taccuracy: 64.2%\n",
            "cost: 0.6070506593320486 \taccuracy: 64.2%\n",
            "cost: 0.6069038447229007 \taccuracy: 64.2%\n",
            "cost: 0.6067579890244628 \taccuracy: 64.4%\n",
            "cost: 0.6066130793907891 \taccuracy: 64.4%\n",
            "cost: 0.6064691032231535 \taccuracy: 64.4%\n",
            "cost: 0.6063260481640106 \taccuracy: 64.4%\n",
            "cost: 0.6061839020911352 \taccuracy: 64.4%\n",
            "cost: 0.606042653111933 \taccuracy: 64.4%\n",
            "cost: 0.6059022895579194 \taccuracy: 64.4%\n",
            "cost: 0.605762799979357 \taccuracy: 64.4%\n",
            "cost: 0.6056241731400493 \taccuracy: 64.4%\n",
            "cost: 0.6054863980122843 \taccuracy: 64.7%\n",
            "cost: 0.6053494637719209 \taccuracy: 64.7%\n",
            "cost: 0.6052133597936178 \taccuracy: 64.7%\n",
            "cost: 0.6050780756461955 \taccuracy: 64.7%\n",
            "cost: 0.6049436010881297 \taccuracy: 64.7%\n",
            "cost: 0.6048099260631702 \taccuracy: 64.7%\n",
            "cost: 0.6046770406960825 \taccuracy: 64.7%\n",
            "cost: 0.6045449352885078 \taccuracy: 64.7%\n",
            "cost: 0.6044136003149361 \taccuracy: 65.0%\n",
            "cost: 0.6042830264187904 \taccuracy: 65.0%\n",
            "cost: 0.6041532044086177 \taccuracy: 65.0%\n",
            "cost: 0.6040241252543842 \taccuracy: 65.0%\n",
            "cost: 0.603895780083869 \taccuracy: 65.0%\n",
            "cost: 0.6037681601791564 \taccuracy: 65.0%\n",
            "cost: 0.6036412569732209 \taccuracy: 65.0%\n",
            "cost: 0.6035150620466049 \taccuracy: 65.0%\n",
            "cost: 0.6033895671241819 \taccuracy: 65.0%\n",
            "cost: 0.603264764072007 \taccuracy: 65.0%\n",
            "cost: 0.6031406448942506 \taccuracy: 65.0%\n",
            "cost: 0.6030172017302091 \taccuracy: 65.0%\n",
            "cost: 0.6028944268513979 \taccuracy: 65.0%\n",
            "cost: 0.6027723126587154 \taccuracy: 65.0%\n",
            "cost: 0.602650851679683 \taccuracy: 65.0%\n",
            "cost: 0.6025300365657545 \taccuracy: 65.0%\n",
            "cost: 0.6024098600896942 \taccuracy: 65.0%\n",
            "cost: 0.6022903151430228 \taccuracy: 65.0%\n",
            "cost: 0.6021713947335267 \taccuracy: 65.0%\n",
            "cost: 0.6020530919828301 \taccuracy: 65.3%\n",
            "cost: 0.6019354001240292 \taccuracy: 65.3%\n",
            "cost: 0.6018183124993844 \taccuracy: 65.6%\n",
            "cost: 0.6017018225580696 \taccuracy: 65.6%\n",
            "cost: 0.6015859238539788 \taccuracy: 65.6%\n",
            "cost: 0.6014706100435844 \taccuracy: 65.3%\n",
            "cost: 0.6013558748838506 \taccuracy: 65.3%\n",
            "cost: 0.6012417122301961 \taccuracy: 65.6%\n",
            "cost: 0.6011281160345062 \taccuracy: 65.6%\n",
            "cost: 0.6010150803431953 \taccuracy: 65.6%\n",
            "cost: 0.6009025992953129 \taccuracy: 65.6%\n",
            "cost: 0.6007906671206976 \taccuracy: 65.6%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}